{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dimahdera/PremiUm-CNN-CIFAR10-Tensorflow-2.x/blob/main/1D_Conv_Bayesian.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hM3SvuCmkQkC"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, initializers, regularizers, constraints\n",
        "\n",
        "class Deterministic_Conv1D(layers.Layer):\n",
        "    def __init__(self, filters, kernel_size, strides=1, padding='valid',\n",
        "                 activation=None,\n",
        "                 kernel_initializer='glorot_uniform',\n",
        "                 kernel_regularizer=None,\n",
        "                 **kwargs):\n",
        "        super(Deterministic_Conv1D, self).__init__(**kwargs)\n",
        "        self.filters = filters\n",
        "        self.kernel_size = kernel_size\n",
        "        self.strides = strides\n",
        "        self.padding = padding.lower()\n",
        "        self.activation = tf.keras.activations.get(activation)\n",
        "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
        "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # Input shape: (batch_size, sequence_length, input_channels)\n",
        "        input_channels = input_shape[-1]\n",
        "        # Define kernel weights\n",
        "        self.kernel = self.add_weight(\n",
        "            shape=(self.kernel_size, input_channels, self.filters),\n",
        "            initializer=self.kernel_initializer,\n",
        "            regularizer=self.kernel_regularizer,\n",
        "            trainable=True,\n",
        "            name='kernel'\n",
        "        )\n",
        "        # Padding logic for 'same'\n",
        "        if self.padding == 'same':\n",
        "            self.pad_size = (self.kernel_size - 1) // 2\n",
        "        else:\n",
        "            self.pad_size = 0\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Apply padding if needed\n",
        "        if self.padding == 'same':\n",
        "            inputs = tf.pad(inputs, [[0, 0], [self.pad_size, self.pad_size], [0, 0]])\n",
        "        # Perform convolution using tf.nn.conv1d\n",
        "        outputs = tf.nn.conv1d(inputs, self.kernel, stride=self.strides, padding='VALID')\n",
        "        # Apply activation function if specified\n",
        "        if self.activation:\n",
        "            outputs = self.activation(outputs)\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Bayesian_Conv1D_first(layers.Layer):\n",
        "    def __init__(self, filters, kernel_size, strides=1, padding='VALID',\n",
        "                 kernel_initializer='glorot_uniform',\n",
        "                 kernel_regularizer=None,\n",
        "                 **kwargs):\n",
        "        super(Bayesian_Conv1D_first, self).__init__(**kwargs)\n",
        "        self.filters = filters\n",
        "        self.kernel_size = kernel_size\n",
        "        self.strides = strides\n",
        "        self.padding = padding\n",
        "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
        "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # Input shape: (batch_size, sequence_length, input_channels)\n",
        "        input_channels = input_shape[-1]\n",
        "        ini_sigma = -2.25\n",
        "        min_sigma = -4.6\n",
        "        # Define kernel weights\n",
        "        self.kernel = self.add_weight(\n",
        "            shape=(self.kernel_size, input_channels, self.filters),\n",
        "            initializer=self.kernel_initializer,\n",
        "            regularizer=self.kernel_regularizer,\n",
        "            trainable=True,\n",
        "            name='kernel'\n",
        "        )\n",
        "        self.kernel_sigma = self.add_weight(shape=(self.filters,), initializer=tf.random_uniform_initializer(minval=min_sigma, maxval=ini_sigma,  seed=None))\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Perform convolution using tf.nn.conv1d\n",
        "        outputs = tf.nn.conv1d(inputs, self.kernel, stride=self.strides, padding=self.padding)\n",
        "        xxT = tf.nn.conv1d(inputs*inputs, tf.constant(1.,shape= self.kernel.shape), stride=self.strides, padding=self.padding ) # shape=[batch_size, t, #kernels]=[100,24,32]\n",
        "        sigma_out = tf.multiply(tf.math.log(1. + tf.math.exp(self.kernel_sigma) ), xxT)\n",
        "        kl_conv = kl_regularizer_conv(self.kernel, self.kernel_sigma)\n",
        "        sigma_out = tf.math.softplus(sigma_out)\n",
        "        return outputs, sigma_out, kl_conv"
      ],
      "metadata": {
        "id": "MoOfvoqBkrrc"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def kl_regularizer_conv(mu, logvar):\n",
        "#    k = mu.shape[-1]\n",
        "#    mu = tf.reshape(mu, [-1, k])\n",
        "#    n= mu.shape[0]\n",
        "    prior_var = 0.01\n",
        "    kl_loss = tf.math.log(prior_var)  - 1 - logvar + (tf.math.log(1+tf.math.exp(logvar))/prior_var) + ( tf.square(mu)/prior_var)\n",
        "    kl = 0.5*tf.math.reduce_mean( kl_loss)#/( tf.math.reduce_max( kl_loss) + tf.keras.backend.epsilon() )\n",
        "   # kl = tf.where(tf.math.is_nan(kl), tf.constant(1.0e-5, shape=kl.shape), kl)\n",
        "   # kl = tf.where(tf.math.is_inf(kl), tf.constant(1.0e-5, shape=kl.shape), kl)\n",
        "    return kl\n",
        "\n",
        "class Bayesian_Conv1D_intermidiate(layers.Layer):\n",
        "    def __init__(self, filters, kernel_size, strides=1, padding='VALID',\n",
        "                 kernel_initializer='glorot_uniform',\n",
        "                 kernel_regularizer=None,\n",
        "                 **kwargs):\n",
        "        super(Bayesian_Conv1D_intermidiate, self).__init__(**kwargs)\n",
        "        self.filters = filters\n",
        "        self.kernel_size = kernel_size\n",
        "        self.strides = strides\n",
        "        self.padding = padding\n",
        "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
        "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        mu_shape, sigma_shape = input_shape\n",
        "        # Input shape: (batch_size, sequence_length, input_channels)\n",
        "        input_channels = mu_shape[-1]\n",
        "        ini_sigma = -2.25\n",
        "        min_sigma = -4.6\n",
        "        # Define kernel weights\n",
        "        self.kernel = self.add_weight(\n",
        "            shape=(self.kernel_size, input_channels, self.filters),\n",
        "            initializer=self.kernel_initializer,\n",
        "            regularizer=self.kernel_regularizer,\n",
        "            trainable=True,\n",
        "            name='kernel'\n",
        "        )\n",
        "        self.kernel_sigma = self.add_weight(shape=(self.filters,), initializer=tf.random_uniform_initializer(minval=min_sigma, maxval=ini_sigma,  seed=None))\n",
        "\n",
        "    def call(self, inputs):\n",
        "        mu_input, sigma_input = inputs\n",
        "        batch_size = mu_input.shape[0]\n",
        "        in_channel = mu_input.shape[-1]\n",
        "        kernel_sigma2 =   tf.math.log(1+tf.math.exp(self.kernel_sigma) )\n",
        "        # Perform convolution using tf.nn.conv1d\n",
        "\n",
        "        outputs = tf.nn.conv1d(mu_input, self.kernel, stride=self.strides, padding=self.padding)\n",
        "\n",
        "        mu_input = tf.expand_dims(mu_input, axis=1) #[batch, 1, sequence_length, input_channels]\n",
        "        sigma_input = tf.expand_dims(sigma_input, axis=1) #[batch, 1, sequence_length, input_channels]\n",
        "\n",
        "        diag_sigma_patches = tf.image.extract_patches(sigma_input, sizes=[1, 1, self.kernel_size, 1],\n",
        "                                                      strides=[1, 1, self.strides, 1],\n",
        "                                                      rates=[1, 1, 1, 1], padding=self.padding) # shape= [batch, 1, new_sequence_length, kernel_size * input_channels]\n",
        "        diag_sigma_g = tf.squeeze(diag_sigma_patches) # shape= [batch, new_sequence_length, kernel_size * input_channels]\n",
        "        mu_cov_square = tf.reshape(tf.math.multiply(self.kernel, self.kernel), [self.kernel_size * in_channel, self.filters])  # shape[ kernel_size*input_channels,   kernel_num]\n",
        "        mu_dim = tf.cast(tf.shape(mu_cov_square)[0], tf.float32)\n",
        "        mu_wT_sigmag_mu_w = tf.matmul(diag_sigma_g, mu_cov_square)/mu_dim  # shape=[batch_size, new_sequence_length , kernel_num   ]\n",
        "        trace = tf.math.reduce_sum(diag_sigma_g, -1, keepdims=True)  # shape=[batch_size,  new_sequence_length, 1]\n",
        "        trace = tf.ones([1, 1, self.filters]) * trace  # shape=[batch_size,  new_sequence_length, kernel_num]\n",
        "\n",
        "\n",
        "        trace =tf.multiply(kernel_sigma2, trace) /mu_dim # shape=[batch_size, , new_im_size*new_im_size, kernel_num]\n",
        "        mu_in_patches = tf.squeeze(tf.image.extract_patches(mu_input, sizes=[1, 1, self.kernel_size, 1],\n",
        "                                                            strides=[1, 1, self.strides, 1],\n",
        "                                                            rates=[1, 1, 1, 1], padding=self.padding)) # shape=[batch_size, new_sequence_length, self.kernel_size*input_channels]\n",
        "\n",
        "        mu_gT_mu_g = tf.math.reduce_sum(tf.math.multiply(mu_in_patches, mu_in_patches), axis=-1)  # shape=[batch_size, new_sequence_length]\n",
        "        mu_gT_mu_g1 = tf.ones([1, 1,  self.filters]) * tf.expand_dims(mu_gT_mu_g, axis=-1)     # shape=[batch_size, new_sequence_length, kernel_num]\n",
        "        sigmaw_mu_gT_mu_g = tf.multiply(kernel_sigma2, mu_gT_mu_g1) /mu_dim  # shape=[batch_size, new_sequence_length, kernel_num]\n",
        "        Sigma_out = trace + mu_wT_sigmag_mu_w + sigmaw_mu_gT_mu_g  # # shape=[batch_size, new_sequence_length, kernel_num]\n",
        "        Sigma_out = tf.math.softplus(Sigma_out)\n",
        "        kl_conv = kl_regularizer_conv(self.kernel, self.kernel_sigma)\n",
        "        return outputs, Sigma_out, kl_conv"
      ],
      "metadata": {
        "id": "wMRW4Ho8HEqd"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch = 2\n",
        "sequence_length = 15\n",
        "input_channels = 3\n",
        "filter_width = 3\n",
        "output_channels = 4\n",
        "stride = 2\n",
        "\n",
        "\n",
        "layer = Bayesian_Conv1D_first(output_channels, filter_width)\n",
        "layer2 = Bayesian_Conv1D_intermidiate(output_channels, filter_width)\n",
        "# Input tensor: [batch, sequence_length, input_channels]\n",
        "input_tensor = tf.random.normal([batch, sequence_length, input_channels])\n",
        "\n",
        "# Filters: [filter_width, input_channels, output_channels]\n",
        "filters = tf.random.normal([filter_width, input_channels, output_channels])\n",
        "\n",
        "# Perform custom 1D convolution\n",
        "output1 = layer(input_tensor)\n",
        "outputs, Sigma_out, kl_conv = output1\n",
        "# Print output shape\n",
        "print(\"Output shape:\", outputs.shape)\n",
        "output2 = layer2((outputs, Sigma_out))\n",
        "outputs2, Sigma_out2, kl_conv2 = output2\n",
        "\n",
        "print(\"Output shape:\", outputs2.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NlGEkSF9k1-n",
        "outputId": "2055ba2d-08e5-4e70-fe51-29c3fc071998"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: (2, 13, 4)\n",
            "Output shape: (2, 11, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Bayesian_Batch_Normalization(layers.Layer):\n",
        "    def __init__(self, var_epsilon):\n",
        "        super(Bayesian_Batch_Normalization, self).__init__()\n",
        "        self.var_epsilon = var_epsilon\n",
        "\n",
        "    def call(self, mu_in, Sigma_in):\n",
        "        mean, variance = tf.nn.moments(mu_in, [0, 1, 2])\n",
        "        mu_out = tf.nn.batch_normalization(mu_in, mean, variance, offset=None, scale=None,\n",
        "                                           variance_epsilon=self.var_epsilon)\n",
        "        Sigma_out = tf.multiply(Sigma_in, 1 / (variance + self.var_epsilon))\n",
        "        return mu_out, Sigma_out"
      ],
      "metadata": {
        "id": "0OHwqy7EC5xG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# with sfiting and scaling learnable parameters\n",
        "class Bayes_BatchNorm(layers.Layer):\n",
        "    def __init__(self, momentum=0.99, eps=1e-6, **kwargs):\n",
        "        \"\"\"\n",
        "        Custom Batch Normalization layer.\n",
        "        Args:\n",
        "            momentum (float): Momentum for the moving average.\n",
        "            eps (float): Small constant for numerical stability.\n",
        "        \"\"\"\n",
        "        self.momentum = momentum\n",
        "        self.eps = eps\n",
        "        super(Bayes_BatchNorm, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        input_shape_mu, input_shape_sigma = input_shape\n",
        "        # Gamma (scale) and beta (shift) parameters\n",
        "        self.gamma = self.add_weight(\n",
        "            shape=input_shape_mu[-1:],\n",
        "            initializer=tf.keras.initializers.Ones(),\n",
        "            name='gamma',\n",
        "            trainable=True\n",
        "        )\n",
        "        self.beta = self.add_weight(\n",
        "            shape=input_shape_mu[-1:],\n",
        "            initializer=tf.keras.initializers.Zeros(),\n",
        "            name='beta',\n",
        "            trainable=True\n",
        "        )\n",
        "        # Moving mean and variance for inference\n",
        "        self.moving_mean = self.add_weight(\n",
        "            shape=input_shape_mu[-1:],\n",
        "            initializer=tf.keras.initializers.Zeros(),\n",
        "            name='moving_mean',\n",
        "            trainable=False\n",
        "        )\n",
        "        self.moving_variance = self.add_weight(\n",
        "            shape=input_shape_mu[-1:],\n",
        "            initializer=tf.keras.initializers.Ones(),\n",
        "            name='moving_variance',\n",
        "            trainable=False\n",
        "        )\n",
        "        super(Bayes_BatchNorm, self).build(input_shape)\n",
        "\n",
        "    def call(self, input, training=None):\n",
        "        \"\"\"\n",
        "        Forward pass of the batch normalization layer.\n",
        "\n",
        "        Args:\n",
        "            x: Input tensor.\n",
        "            training (bool): If True, the layer is in training mode.\n",
        "\n",
        "        Returns:\n",
        "            Batch-normalized tensor.\n",
        "        \"\"\"\n",
        "        x, sigma_x = input\n",
        "        if training:\n",
        "            # Calculate batch mean and variance\n",
        "            batch_mean = tf.reduce_mean(x, axis=0, keepdims=False)\n",
        "            batch_variance = tf.reduce_variance(x, axis=0, keepdims=False)\n",
        "\n",
        "            # Update moving mean and variance\n",
        "            self.moving_mean.assign(\n",
        "                self.momentum * self.moving_mean + (1.0 - self.momentum) * batch_mean\n",
        "            )\n",
        "            self.moving_variance.assign(\n",
        "                self.momentum * self.moving_variance + (1.0 - self.momentum) * batch_variance\n",
        "            )\n",
        "\n",
        "            mean, variance = batch_mean, batch_variance\n",
        "\n",
        "        else:\n",
        "            # Use moving mean and variance for inference\n",
        "            mean, variance = self.moving_mean, self.moving_variance\n",
        "\n",
        "        # Normalize input\n",
        "        x_normalized = (x - mean) / tf.sqrt(variance + self.eps)\n",
        "        x_normalized = self.gamma * x_normalized + self.beta\n",
        "        a = (self.gamma / (variance + self.eps)) ** 2  # [50,17,64]\n",
        "        Sigma_out = tf.math.multiply(a, sigma_x)  # [50,17,64]\n",
        "        Sigma_out = tf.math.softplus(Sigma_out)\n",
        "        # Scale and shift\n",
        "        return x_normalized, Sigma_out\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape"
      ],
      "metadata": {
        "id": "Qtx3xRAXNamr"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch = 2\n",
        "sequence_length = 15\n",
        "input_channels = 3\n",
        "filter_width = 3\n",
        "output_channels = 4\n",
        "stride = 2\n",
        "\n",
        "\n",
        "layer = Bayesian_Conv1D_first(output_channels, filter_width)\n",
        "layer2 = Bayesian_Conv1D_intermidiate(output_channels, filter_width)\n",
        "layer3 = Bayes_BatchNorm()\n",
        "# Input tensor: [batch, sequence_length, input_channels]\n",
        "input_tensor = tf.random.normal([batch, sequence_length, input_channels])\n",
        "\n",
        "# Filters: [filter_width, input_channels, output_channels]\n",
        "filters = tf.random.normal([filter_width, input_channels, output_channels])\n",
        "\n",
        "# Perform custom 1D convolution\n",
        "output1 = layer(input_tensor)\n",
        "outputs, Sigma_out, kl_conv = output1\n",
        "\n",
        "# Print output shape\n",
        "print(\"Output shape:\", outputs.shape)\n",
        "output2 = layer2((outputs, Sigma_out))\n",
        "outputs2, Sigma_out2, kl_conv2 = output2\n",
        "print(\"Output shape:\", outputs2.shape)\n",
        "\n",
        "output3 = layer3((outputs2, Sigma_out2))\n",
        "outputs3, Sigma_out3 = output3\n",
        "print(\"Output shape:\", outputs3.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQHACcTUSOYC",
        "outputId": "53b47c3f-4fa3-4f94-d383-f59eb8369343"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: (2, 13, 4)\n",
            "Output shape: (2, 11, 4)\n",
            "Output shape: (2, 11, 4)\n"
          ]
        }
      ]
    }
  ]
}